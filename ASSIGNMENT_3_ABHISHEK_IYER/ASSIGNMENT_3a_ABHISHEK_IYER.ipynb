{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3a\n",
    "\n",
    "## Due: Friday, September 30, 2022 at 5pm (submission via Canvas)\n",
    "\n",
    "\n",
    "* Please submit your assignment (notebooks of parts 3a and 3b + Python modules) as **a single .zip file** using Canvas (Assignments --> Assignment 3). Please put the notebooks for Assignment 3a and 3b as well as the Python modules (files ending with .py) in one folder, which you call ASSIGNMENT_3_FIRSTNAME_LASTNAME. Please zip this folder and upload it as your submission.\n",
    "\n",
    "* Please name your zip file with the following naming convention: ASSIGNMENT_3_FIRSTNAME_LASTNAME.zip\n",
    "\n",
    "**IMPORTANTE NOTE**:\n",
    "* The students who follow the Bachelor version of this course, i.e., the course Introduction to Python for Humanities and Social Sciences (L_AABAALG075) as part of the minor Digital Humanities, do **not have to do Exercises 3 and 4 of Assignment 3b**\n",
    "* The other students, i.e., who follow the Master version of  course, which is Programming in Python for Text Analysis (L_AAMPLIN021), are required to **do Exercises 3 and 4 of Assignment 3b**\n",
    "\n",
    "If you have **questions** about this topic, please contact us **(cltl.python.course@gmail.com)**. Questions and answers will be collected on Piazza, so please check if your question has already been answered first.\n",
    "\n",
    "\n",
    "In this block, we covered a lot of ground:\n",
    "\n",
    "* Chapter 12 - Importing external modules \n",
    "* Chapter 13 - Working with Python scripts\n",
    "* Chapter 14 - Reading and writing text files\n",
    "* Chapter 15 - Off to analyzing text \n",
    "\n",
    "\n",
    "In this assignment, you will first complete a number of small exercises about each chapter to make sure you are familiar with the most important concepts. In the second part of the assignment, you will apply your newly acquired skills to write your very own text processing program (ASSIGNMENT-3b) :-). But don't worry, there will be instructions and hints along the way. \n",
    "\n",
    "\n",
    "**Can I use external modules other than the ones treated so far?**\n",
    "\n",
    "For now, please try to avoid it. All the exercises can be solved with what we have covered in block I, II, and III. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions & scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1:\n",
    "\n",
    "Define a function called `split_sort_text` which takes one positional parameter called **text** (a string).\n",
    "\n",
    "The function:\n",
    "* splits the string on a space character, i.e., ' '\n",
    "* returns all the unique words in alphabetical order as a list.\n",
    "\n",
    "* Hint 1: There is a specific python container which does not allow for duplicates and simply removes them. Use this one. \n",
    "* Hint 2: There is a function which sorts items in an iterable called 'sorted'. Look at the documentation to see how it is used. \n",
    "* Hint 3: Don't forget to write a docstring. Please make sure that the docstring generally explains with the input is, what the function does, and what the function returns. If you want, but this is not needed to receive full points, you can use [reStructuredText](http://docutils.sourceforge.net/rst.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the desired string I am abhishek\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ABHISHEK', 'AM', 'I']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_sort_text(text):\n",
    "    \"\"\"\n",
    "    The function to split the user input string on whitespace and return all the unique words in alphabetical order as a list.\n",
    "\n",
    "    :param text: this is the positional parameter which takes the user input string\n",
    "    :returns: all the unique words in alphabetical order as a list.\n",
    "    \n",
    "    \"\"\"\n",
    "    word=text.upper()\n",
    "    word=word.split()\n",
    "    sorted_word=sorted (set(word))\n",
    "    return sorted_word\n",
    "\n",
    "user_input = str(input(\"Enter the desired string \"))\n",
    "split_sort_text(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with external modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "NLTK offers a way of using WordNet in Python. Do some research (using google, because quite frankly, that's what we do very often) and see if you can find out how to import it. WordNet is a computational lexicon which organizes words according to their senses (collected in synsets). See if you can print all the **synset definitions** of the lemma **dog**.\n",
    "\n",
    "Make sure you have run the following cell to make sure you have installed WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Abhishek\n",
      "[nltk_data]    |     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# uncomment the following line to download material including WordNet\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Abhishek\n",
      "[nltk_data]     Iyer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wo\n",
    "synset = wo.synsets(\"dog\")\n",
    "print(synset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with python scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise  3\n",
    "\n",
    "#### a.) Define a function called `count`, which determines how often each word occurs in a string. Do not use NLTK just yet. Find a way to test it. \n",
    "\n",
    "* Write a helper-function called `preprocess`, which removes the punctuation specified by the user, and returns the same string without the unwanted characters. You call the function `preprocess` inside the `count` function.\n",
    "\n",
    "* Remember that there are string methods that you can use to get rid of unwanted characters. Test the `preprocess` function using the following string `'this is a (tricky) test'`.\n",
    "\n",
    "* Remember how we used dictionaries to count words? If not, have a look at Chapter 10 - Dictionaries. \n",
    "\n",
    "* make sure you split the string on a space character ' '. You loop over the list to count the words.\n",
    "\n",
    "* Test your function using an example string, which will tell you whether it fulfills the requirements (remove punctuation, split, count). You will get a point for good testing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the punctuation you want removed (\n",
      "To continue press y, else press n\n",
      "enter y or n y\n",
      "enter the punctuation you want removed )\n",
      "To continue press y, else press n\n",
      "enter y or n n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'this': 1, 'is': 1, 'a': 1, 'tricky': 1, 'test': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text,punctuations):\n",
    "    \"\"\"\n",
    "    This method removes the user given characters from text\n",
    "\n",
    "    :param text: string\n",
    "    :param punctuations: string\n",
    "    :returns: text after removing punctuations (user given characters)\n",
    "    \"\"\"\n",
    "    return text.replace(punctuations,\"\")\n",
    "\n",
    "def count(text):\n",
    "    \"\"\"\n",
    "    This method counts the words in text after removing the user given punctuations.\n",
    "    Instruction for giving input: Enter character and press enter. If input is to be \n",
    "    stopped enter n when the question is asked, else enter y to keep inputting characters.\n",
    "\n",
    "    :param text: string\n",
    "    :returns: count of words in the given text\n",
    "    \"\"\"\n",
    "\n",
    "    punctuations = []\n",
    "    user_input = \" \"\n",
    "    exit_char = \" \"\n",
    "    while exit_char != \"n\":\n",
    "        user_input = input(\"enter the punctuation you want removed \")\n",
    "        punctuations.append(user_input)\n",
    "        print(\"To continue press y, else press n\")\n",
    "        exit_char = input(\"enter y or n \")\n",
    "    for punctuation in punctuations:\n",
    "        text=preprocess(text,punctuation)\n",
    "    \n",
    "    words = text.split()\n",
    "    counts = {}\n",
    "    for word in words:\n",
    "        counts[word]=words.count(word)\n",
    "    return counts\n",
    "\n",
    "count(\"this is a (tricky) test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.) Create a python script \n",
    "\n",
    "Use your editor to create a Python script called **count_words.py**. Place the function definition of the **count** function in **count_words.py**. Also put a function call of the **count** function in this file to test it. Place your helper function definition, i.e., **preprocess**, in a separate script called **utils_3a.py**. Import your helper function **preprocess** into count_words.py. Test whether everything works as expected by calling the script count_words.py from the terminal.\n",
    "\n",
    "The function **preprocess** preprocesses the text by removing characters that are unwanted by the user. **preprocess** is called within the **count** function and hence builds upon the output from the preprocess function and creates a dictionary in which the key is a word and the value is the frequency of the word.\n",
    "\n",
    "**Please submit these scripts together with the other notebooks**.\n",
    "\n",
    "Don't forget to add docstrings to your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the punctuation you want removed (\n",
      "To continue press y, else press n\n",
      "enter y or n y\n",
      "enter the punctuation you want removed )\n",
      "To continue press y, else press n\n",
      "enter y or n n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'this': 1, 'is': 1, 'a': 1, 'tricky': 1, 'test': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from count_words import count\n",
    "count(\"this is a tricky test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "**Playing with lyrics**\n",
    "\n",
    "a.) Write a function called `load_text`, which opens and reads a file and returns the text in the file. It should have the file path as a parameter. Test it by loading this file: ../Data/lyrics/walrus.txt\n",
    "\n",
    "* Hint: remember it is best practice to use a context manager\n",
    "* Hint: **FileNotFoundError**: This means that the path you provide does not lead to an existing file on your computer. Please carefully study Chapter 14. Please determine where the notebook or Python module that you are working with is located on your computer. Try to determine where Python is looking if you provide a path such as “../Data/lyrics/walrus.txt”. Try to go from your notebook to the location on your computer where Python is trying to find the file. One tip: if you did not store the Assignments notebooks 3a and 3b in the folder “Assignments”, you would get this error.\n",
    "\n",
    "b.) Write a function called `replace_walrus`, which takes lyrics as input and replaces every instance of 'walrus' by 'hippo' (make sure to account for upper and lower case - it is fine to transform everything to lower case). The function should write the new version of the song to a file called 'walrus_hippo.txt and stored in ../Data/lyrics. \n",
    "\n",
    "Don't forget to add docstrings to your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"I Am The Walrus\"\\n(\"Magical Mystery Tour\" Version)\\n\\nI am he\\nAs you are he\\nAs you are me\\nAnd we are all together\\n\\nSee how they run\\nLike pigs from a gun\\nSee how they fly\\nI\\'m crying\\n\\nSitting on a cornflake\\nWaiting for the van to come\\nCorporation tee shirt\\nStupid bloody Tuesday\\nMan, you been a naughty boy\\nYou let your face grow long\\n\\nI am the eggman (Ooh)\\nThey are the eggmen, (Ooh)\\nI am the walrus\\nGoo goo g\\' joob\\n\\nMister city p\\'liceman sitting pretty\\nLittle p\\'licemen in a row\\nSee how they fly\\nLike Lucy in the sky\\nSee how they run\\nI\\'m crying\\nI\\'m crying, I\\'m crying, I\\'m crying\\n\\nYellow matter custard\\nDripping from a dead dog\\'s eye\\nCrabalocker fishwife pornographic priestess\\nBoy you been a naughty girl\\nYou let your knickers down\\n\\nI am the eggman (Ooh)\\nThey are the eggmen (Ooh)\\nI am the walrus\\nGoo goo g\\' joob\\n\\nSitting in an English\\nGarden waiting for the sun\\nIf the sun don\\'t come\\nYou get a tan from standing in the English rain\\n\\nI am the eggman\\nThey are the eggmen\\nI am the walrus\\nGoo goo g\\' joob g\\' goo goo g\\' joob\\n\\nExpert texpert choking smokers\\nDon\\'t you think the joker laughs at you?\\nSee how they smile\\nLike pigs in a sty, see how they snied\\nI\\'m crying\\n\\nSemolina pilchards\\nClimbing up the Eiffel Tower\\nElement\\'ry penguin singing Hare Krishna\\nMan, you should have seen them kicking Edgar Allan Poe\\n\\nI am the eggman (Ooh)\\nThey are the eggmen (Ooh)\\nI am the walrus\\nGoo goo g\\' joob\\nGoo goo g\\' joob\\nG\\' goo goo g\\' joob\\nGoo goo g\\' joob, goo goo g\\' goo g\\' goo goo g\\' joob joob\\nJoob joob...'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_text(file_path):\n",
    "    \"\"\"\n",
    "    This method loads a lyrics text file\n",
    "    \n",
    "    :param file_path: string\n",
    "    :returns: lyrics read from the text file of the given path\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        return f.read()\n",
    "\n",
    "lyrics=load_text(\"Data/lyrics/walrus.txt\") # changed the path from ../Data as I have included the Data folder \n",
    "                                           # with lyrics folder in the submission document \n",
    "lyrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_walrus(lyrics):\n",
    "    \"\"\"\n",
    "    This method replaces \"walrus\" with \"hippo\" in the given string \n",
    "    and saves it in ../Data/lyrics/walrus_hippo.txt file\n",
    "    \n",
    "    :param lyrics: string\n",
    "    :returns: nothing\n",
    "    \"\"\"\n",
    "\n",
    "    lyrics=lyrics.lower().replace(\"walrus\",\"hippo\")\n",
    "    with open(\"Data/lyrics/walrus_hippo.txt\",\"w\") as text_file: # changed the path from ../Data as I have included the Data folder \n",
    "                                                                # with lyrics folder in the submission document \n",
    "        text_file.write(lyrics)\n",
    "replace_walrus(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing text with nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "**Building a simple NLP pipeline**\n",
    "\n",
    "For this exercise, you will need NLTK. Don't forget to import it. \n",
    "\n",
    "Write a function called `tag_text`, which takes raw text as input and returns the tagged text. To do this, make sure you follow the steps below:\n",
    "\n",
    "* Tokenize the text. \n",
    "\n",
    "* Perform part-of-speech tagging on the list of tokens. \n",
    "\n",
    "* Return the tagged text\n",
    "\n",
    "\n",
    "Then test your function using the text snipped below (`test_text`) as input.\n",
    "\n",
    "Please note that the tags may not be correct and that this is not a mistake on your end, but simply NLP tools not being perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Shall', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('compare', 'VBP'),\n",
       " ('thee', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('a', 'DT'),\n",
       " ('summer', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('day', 'NN'),\n",
       " ('?', '.'),\n",
       " ('Thou', 'NNP'),\n",
       " ('art', 'RB'),\n",
       " ('more', 'RBR'),\n",
       " ('lovely', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('more', 'JJR'),\n",
       " ('temperate', 'NN'),\n",
       " (':', ':'),\n",
       " ('Rough', 'NNP'),\n",
       " ('winds', 'NNS'),\n",
       " ('do', 'VBP'),\n",
       " ('shake', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('darling', 'VBG'),\n",
       " ('buds', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('May', 'NNP'),\n",
       " (',', ','),\n",
       " ('And', 'CC'),\n",
       " ('summer', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('lease', 'NN'),\n",
       " ('hath', 'NN'),\n",
       " ('all', 'DT'),\n",
       " ('too', 'RB'),\n",
       " ('short', 'JJ'),\n",
       " ('a', 'DT'),\n",
       " ('date', 'NN'),\n",
       " (':', ':')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = \"\"\"Shall I compare thee to a summer's day?\n",
    "Thou art more lovely and more temperate:\n",
    "Rough winds do shake the darling buds of May,\n",
    "And summer's lease hath all too short a date:\"\"\"\n",
    "def tag_text(text):\n",
    "    \"\"\"\n",
    "    This method does tokenization and part of speech tagging of the given text\n",
    "    \n",
    "    :param text: string\n",
    "    :returns: list of part of speech tags of the given text\n",
    "    \"\"\"\n",
    "    tokens=nltk.word_tokenize(text)\n",
    "    return nltk.pos_tag(tokens)\n",
    "tag_text(test_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "6.a) Explain in your own words the difference between the global and the local scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables defined within the bounds of a function have local scope and cannot be accessed by any other function in the code. Variables that are declared outside the bounds of any function have global scope and can be accessed throughout the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.b) What is the difference between the modes 'w' and 'a' when opening a file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mode 'w' is over-writing while the mode 'a' is for append. So in mode 'w' we can over-write the contents of a file while in mode 'a' we can append (add) more content to the file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
